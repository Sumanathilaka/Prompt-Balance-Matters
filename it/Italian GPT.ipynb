{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating German WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, re\n",
    "from openai import OpenAI, ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    cadentials = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = cadentials['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_model(USER_MESSAGE):\n",
    "    response = client.chat.completions.create(\n",
    "                                            model = 'gpt-4-turbo',\n",
    "                                            messages = [\n",
    "                                                        {\"role\": \"system\", \"content\" : \"You are a helpful assitant to identify the sense tag for sense of a word in Italian\"},\n",
    "                                                        {\"role\": \"user\", \"content\": USER_MESSAGE}              \n",
    "                                                        ],\n",
    "                                            temperature=0,\n",
    "                                            max_tokens=1500\n",
    "                                            )\n",
    "    return str(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "wordnet_dict={}\n",
    "with open (\"inventory.it.txt\",\"r\") as file:\n",
    "    for i in file:\n",
    "        parts=i.split(\"\\t\")\n",
    "        element=i.split(\"\\t\")[0]\n",
    "        word,pos=element.split(\"#\")[0],element.split(\"#\")[1]\n",
    "        value = \"\\t\".join(parts[1:]).strip()\n",
    "        doc = nlp(word)\n",
    "        lemmatized_words = [token.lemma_ for token in doc]\n",
    "\n",
    "\n",
    "        #print(lemmatized_words[0]+\"\\t\"+pos+\"\\t\"+value)\n",
    "\n",
    "        # meanings = get_babelnet_sense_meaning(sense_id)\n",
    "        # print(meanings[0])\n",
    "\n",
    "        #save all the data in a dictionary keeping the word as key and the senses as values in a list\n",
    "        wordnet_dict[word]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_babelnet_sense_meaning(sense_id):\n",
    "    # Define the BabelNet API endpoint\n",
    "    url = f'https://babelnet.io/v6/getSynset'\n",
    "    \n",
    "    # Define the parameters for the API request\n",
    "    params = {\n",
    "        'id': sense_id,\n",
    "        'key': \"#\"\n",
    "    }\n",
    "    \n",
    "    # Make the API request\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Get the English glosses (definitions)\n",
    "        #print(data)\n",
    "        glosses = data.get('glosses', [])\n",
    "        english_glosses = [gloss['gloss'] for gloss in glosses if gloss['language'] == 'EN']\n",
    "\n",
    "        lemmas = []\n",
    "        for sense in data.get('senses', []):\n",
    "            properties = sense.get('properties', {})\n",
    "            lemma_info = properties.get('lemma', {})\n",
    "            lemma = lemma_info.get('lemma')\n",
    "            if lemma:\n",
    "                lemmas.append(lemma)\n",
    "\n",
    "\n",
    "        senses = data.get('senses', [])\n",
    "        senseKey = None\n",
    "\n",
    "        if senses:\n",
    "            sense_properties = senses[0].get('properties', {})\n",
    "            senseKey = sense_properties.get('senseKey', None)\n",
    "\n",
    "\n",
    "        #print(lemmas)\n",
    "\n",
    "        return english_glosses[0],lemmas[0],senseKey\n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch data (status code: {response.status_code})\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glosses,lemma,sensekey_wn=get_babelnet_sense_meaning(\"bn:00082136v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find repugnant\n",
      "abhor\n",
      "abhor%2:37:00::\n"
     ]
    }
   ],
   "source": [
    "print(glosses)\n",
    "print(lemma)\n",
    "print(sensekey_wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file\n",
    "wordnet = []\n",
    "with open('wordnet.txt', 'r',encoding=\"utf8\") as file:\n",
    "    for element in file:\n",
    "        items=element.strip(\"\\n\").split(\"\t\")\n",
    "        wordnet.append(items)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fourth_element(lists_of_lists, key):\n",
    "    for sublist in lists_of_lists:\n",
    "        if sublist[0] == key:\n",
    "            return sublist[3]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loathe', 'abominate', 'execrate']\n"
     ]
    }
   ],
   "source": [
    "result = extract_fourth_element(wordnet, sensekey_wn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to counts the number of tokens : input and output tokens\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_tokens(text):\n",
    "    # Tokenize the input text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Count the tokens in the input\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_dictionary_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            data_dict = json.load(file)\n",
    "        return data_dict\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the file doesn't exist\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle the case where the file contains invalid JSON\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dict = load_dictionary_from_file(\"my_dictionary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_balanced_examples_low(input_list):\n",
    "    # Parse the input list into a dictionary with senseid as key\n",
    "    senseid_dict = defaultdict(list)\n",
    "    \n",
    "    for item in input_list:\n",
    "        \n",
    "        example, senseid = item.rsplit(',', 1)\n",
    "        senseid_dict[senseid.strip()].append(item.strip())  # Store the full item (example + senseid)\n",
    "    \n",
    "    #Find the minimum number of examples for any senseid\n",
    "    min_examples = min(len(examples) for examples in senseid_dict.values())\n",
    "    \n",
    "    #Randomly sample min_examples from each senseid\n",
    "    balanced_list = []\n",
    "    for senseid, examples in senseid_dict.items():\n",
    "        balanced_list.extend(random.sample(examples, min_examples))\n",
    "    \n",
    "    return balanced_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_balanced_examples(input_list):\n",
    "    # Parse the input list into a dictionary with senseid as key\n",
    "    senseid_dict = defaultdict(list)\n",
    "    \n",
    "    for item in input_list:\n",
    "        \n",
    "        example, senseid = item.rsplit(',', 1)\n",
    "        senseid_dict[senseid.strip()].append(item.strip())  # Store the full item (example + senseid)\n",
    "    \n",
    "    #Find the minimum number of examples for any senseid\n",
    "    avg_examples = min(len(examples) for examples in senseid_dict.values()) + max(len(examples) for examples in senseid_dict.values())\n",
    "    avg_examples=int(avg_examples/2)\n",
    "\n",
    "    \n",
    "    #Randomly sample min_examples from each senseid\n",
    "    balanced_list = []\n",
    "    for senseid, examples in senseid_dict.items():\n",
    "        balanced_list.extend(random.sample(examples, avg_examples))\n",
    "    \n",
    "    return balanced_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying the WSD word from the data \n",
    "def wsdword(text):\n",
    "    match = re.search(r'<WSD>(.*?)<WSD>', text)\n",
    "    if match:\n",
    "        word_inside_wsd = match.group(1)\n",
    "        return word_inside_wsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_example=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sense_Tag_Return_pipeline(sentence,postag,wordwsd,wordnet):\n",
    "    \n",
    "    #retrieving the base form of the sentence\n",
    "    #global no_of_example\n",
    "    doc = nlp(wordwsd)\n",
    "    lemmatized_words = [token.lemma_ for token in doc]\n",
    "    lemmatized_word =lemmatized_words[0].lower()\n",
    "    #print(lemmatized_word)\n",
    "    #retrieving the data form the KB (retriver)\n",
    "    try:\n",
    "        examples=loaded_dict[wordwsd][postag]\n",
    "        #Fewshot bias Removal\n",
    "        #examples=get_balanced_examples_low(examples)\n",
    "        no_of_example+=len(examples)\n",
    "        #print(examples)\n",
    "    except:\n",
    "        examples=\"No Examples\"\n",
    "        #print(examples)\n",
    "\n",
    "    #retrieving the meanings from Bablenet\n",
    "    sense_definition=\"\"\n",
    "    list_senseids=wordnet_dict[lemmatized_word].split(\"\\t\")\n",
    "    print(list_senseids)\n",
    "    for i in list_senseids:\n",
    "        glosses,lemma,sensekey_wn=get_babelnet_sense_meaning(i)\n",
    "        result = extract_fourth_element(wordnet, sensekey_wn)\n",
    "        senseinfo=\"senseid: \"+i+\", gloss :\"+glosses+ \", lemma in English :\"+lemma +\", Synonyms of the word if any :\"+result+\"\\n\"\n",
    "        sense_definition+=senseinfo\n",
    "\n",
    "   \n",
    "    #print(sense_definition)\n",
    "    \n",
    "        \n",
    "\n",
    "    count=0 #variable to count the tokens \n",
    "    \n",
    "    #prompt=f\"Examine the sentence. {instance_meaning}.Return most suitable sense id associated with from below. it contain sense id and it's definition {meanings}. utilize the below examples also to finalize the answer {examples}\"\n",
    "    prompt = f'''You are going to identify the corresponding sense tag of an ambiguous word in italian sentences. Use multiple reasoning strategies to increase confidence in your answer.\n",
    "1. The word \"{wordwsd}\" has different meanings. Below are possible meanings. Comprehend the sense tags and meanings. English lemma and synonyms are provided. {sense_definition}\n",
    "2. You can learn more on the usage of each word and the its sense through the examples below. Each sentence is followed by its corresponding sense id. \"{examples}\"\n",
    "3. Now carefully examine the German sentence below. The ambiguous word is enclosed within <WSD>.\"{sentence}\"\n",
    "4. Analyze the sentence using the following techniques and identify the meaning of the ambiguous word.\n",
    "   Focus on keywords in the sentence surrounding the ambiguous word. \n",
    "   Think about the overall topic and intent of the sentence. Decide on the sense of the word that makes the most logical sense within the context. \n",
    "5. Based on the identified meaning, try to find the most appropriate senseIDs from the above sense tag list. You are given definition of each sense tag too.\n",
    "6. If you have more than one senseIDs identified after above steps, you can return the senseIDs in order of confident level, follow the given format to return the value .\n",
    "7. Return only the finalized senseIDs. Do not add extra detials and explanation. Only senseID is expected.\n",
    "\n",
    "if only one senseid is identified then <senseId>\n",
    "if not <senseId1, senseId2,...> '''\n",
    "    #print(prompt)\n",
    "    count+=count_tokens(prompt)\n",
    "    output=complete_model(prompt)\n",
    "    #output=\"\"\n",
    "    count+=count_tokens(output)\n",
    "    return output, count    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "#evaluating the results\n",
    "\n",
    "csv_file_path = 'test/sem13_it.csv'\n",
    "totalTokens=0\n",
    "# Open the CSV file\n",
    "with open(csv_file_path, mode='r', newline='', encoding=\"utf-8\") as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    \n",
    "    # Iterate over each row in the CSV file\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        # Access the columns by their header names\n",
    "        sentence = row['sentence']\n",
    "        fileid = row['fileid']\n",
    "        senseid = row['senseid'].strip()        \n",
    "        \n",
    "        try:\n",
    "            postag=senseid[-1]  #postag \n",
    "            wordfromtext=wsdword(sentence)\n",
    "            #print(wordfromtext+\" \"+senseid+\" \"+postag+\" \"+sentence)            \n",
    "            output,token_count=sense_Tag_Return_pipeline(sentence,postag,wordfromtext.lower(),wordnet)\n",
    "            #sense_Tag_Return_pipeline(sentence,postag,wordfromtext)\n",
    "            totalTokens+=token_count\n",
    "            print(output)\n",
    "            \n",
    "        except:\n",
    "            print()\n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "file.close()\n",
    "\n",
    "print(no_of_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import csv\n",
    "\n",
    "# #evaluating the results\n",
    "\n",
    "# csv_file_path = 'test/sem13_it.csv'\n",
    "# totalTokens=0\n",
    "# # Open the CSV file\n",
    "# with open(csv_file_path, mode='r', newline='', encoding=\"utf-8\") as file:\n",
    "#     # Create a CSV reader object\n",
    "#     csv_reader = csv.DictReader(file)\n",
    "    \n",
    "#     # Iterate over each row in the CSV file\n",
    "    \n",
    "#     for row in csv_reader:\n",
    "#         # Access the columns by their header names\n",
    "#         sentence = row['sentence']\n",
    "#         fileid = row['fileid']\n",
    "#         senseid = row['senseid'].strip()        \n",
    "        \n",
    "#         try:\n",
    "#             postag=senseid[-1]  #postag \n",
    "#             wordfromtext=wsdword(sentence)\n",
    "#             #print(wordfromtext+\" \"+senseid+\" \"+postag+\" \"+sentence)            \n",
    "#             output,token_count=sense_Tag_Return_pipeline(sentence,postag,wordfromtext.lower(),wordnet)\n",
    "#             #sense_Tag_Return_pipeline(sentence,postag,wordfromtext)\n",
    "#             totalTokens+=token_count\n",
    "#             print(output)\n",
    "            \n",
    "#         except:\n",
    "#             print()\n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "# file.close()\n",
    "\n",
    "# print(totalTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import csv\n",
    "\n",
    "# #evaluating the results\n",
    "\n",
    "# csv_file_path = 'test/sem13_it.csv'\n",
    "# totalTokens=0\n",
    "# # Open the CSV file\n",
    "# with open(csv_file_path, mode='r', newline='', encoding=\"utf-8\") as file:\n",
    "#     # Create a CSV reader object\n",
    "#     csv_reader = csv.DictReader(file)\n",
    "    \n",
    "#     # Iterate over each row in the CSV file\n",
    "    \n",
    "#     for row in csv_reader:\n",
    "#         # Access the columns by their header names\n",
    "#         sentence = row['sentence']\n",
    "#         fileid = row['fileid']\n",
    "#         senseid = row['senseid'].strip()        \n",
    "        \n",
    "#         try:\n",
    "#             postag=senseid[-1]  #postag \n",
    "#             wordfromtext=wsdword(sentence)\n",
    "#             #print(wordfromtext+\" \"+senseid+\" \"+postag+\" \"+sentence)            \n",
    "#             output,token_count=sense_Tag_Return_pipeline(sentence,postag,wordfromtext.lower(),wordnet)\n",
    "#             #sense_Tag_Return_pipeline(sentence,postag,wordfromtext)\n",
    "#             totalTokens+=token_count\n",
    "#             print(output)\n",
    "            \n",
    "#         except:\n",
    "#             print()\n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "# file.close()\n",
    "\n",
    "# print(totalTokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
