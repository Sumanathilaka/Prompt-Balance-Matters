{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating a Prompt using Majority Vote Strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, re\n",
    "from openai import OpenAI, ChatCompletion\n",
    "import together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for accessing together/ai API\n",
    "with open('credentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "os.environ['TOGETHER_AI_API'] = credentials['TOGETHER_AI_API']\n",
    "together.api_key = os.environ[\"TOGETHER_AI_API\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import base64\n",
    "from openai import AzureOpenAI  \n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "\n",
    "endpoint = os.getenv(\"#\")  \n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")  \n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"#\")  \n",
    "\n",
    "# Initialize Azure OpenAI Service client with key-based authentication    \n",
    "client = AzureOpenAI(  \n",
    "    azure_endpoint=endpoint,  \n",
    "    api_key=subscription_key,  \n",
    "    api_version = \"2024-12-01-preview\"\n",
    ")\n",
    "\n",
    "    \n",
    "# Generate the completion  \n",
    "def chat_completion(USER_MSG):\n",
    "    completion = client.chat.completions.create(  \n",
    "        model=deployment,\n",
    "        messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are a helpful assitant to identify the tag for sense of a word\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": USER_MSG\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "] ,\n",
    "        max_tokens=500,  \n",
    "        temperature=0.0,\n",
    "        top_p=0.95,  \n",
    "        frequency_penalty=0,  \n",
    "        presence_penalty=0,\n",
    "        stop=None,  \n",
    "        stream=False\n",
    "        )\n",
    "    response_content = completion.choices[0].message.content\n",
    "    return response_content\n",
    "\n",
    "# Generate the completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credentials.yaml') as f:\n",
    "    cadentials = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = cadentials['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI() #for OpenAI API\n",
    "from together import Together #for Together AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_model(USER_MESSAGE):\n",
    "    output = together.Complete.create(\n",
    "                                    prompt= USER_MESSAGE,\n",
    "                                    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "                                    max_tokens=1200,\n",
    "                                    temperature=0,\n",
    "                                    \n",
    "                                    \n",
    "                                    )\n",
    "    text = output['output']['choices'][0]['text']\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_model(USER_MESSAGE):\n",
    "    response = client.chat.completions.create(\n",
    "                                            model = 'gpt-4o',\n",
    "                                            messages = [\n",
    "                                                        {\"role\": \"system\", \"content\" : \"You are a helpful assitant to identify the tag for sense of a word\"},\n",
    "                                                        {\"role\": \"user\", \"content\": USER_MESSAGE}              \n",
    "                                                        ],\n",
    "                                            temperature=0,\n",
    "                                            max_tokens=1500\n",
    "                                            )\n",
    "    return str(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file\n",
    "list_of_lists = []\n",
    "with open('wordnet.txt', 'r',encoding=\"utf8\") as file:\n",
    "    for element in file:\n",
    "        items=element.strip(\"\\n\").split(\"\t\")\n",
    "        list_of_lists.append(items)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the word into the base word as the sense are kept in base\n",
    "def word_base(word):\n",
    "    try:\n",
    "        word=stemmer.stem(word)\n",
    "        base_word = lemmatizer.lemmatize(word)\n",
    "        return base_word\n",
    "    except:\n",
    "        return word   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to retrieve the word meaning from the list_of_list list\n",
    "#this function will specifically read the sense id and the meaning(gloss) which is required for the processing.\n",
    "def retrieve_meanings(word, data):\n",
    "    meanings_dict = {}\n",
    "    for entry in data:\n",
    "        if word == entry[1].split(\".\")[0]:\n",
    "            if entry[-1] != 'None':\n",
    "                meanings_dict[entry[1]] = entry[2]+\", synonyms :\"+entry[-1]+\"\\n\"\n",
    "            else:\n",
    "                meanings_dict[entry[1]] = entry[2]+\"\\n\"\n",
    "    return meanings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use.n.04:00': \"the act of using, synonyms :['usage', 'utilization', 'utilisation', 'employment', 'exercise']\\n\", 'use.n.04:01': \"(psychology) an automatic pattern of behavior in reaction to a specific situation; may be inherited or acquired through frequent repetition, synonyms :['habit']\\n\", 'use.n.04:02': \"exerting shrewd or devious influence especially for one's own advantage, synonyms :['manipulation']\\n\", 'use.n.07:00': \"what something is used for, synonyms :['function', 'purpose', 'role']\\n\", 'use.n.07:02': 'a particular service, synonyms :[]\\n', 'use.n.07:03': \"(law) the exercise of the legal right to enjoy the benefits of owning property, synonyms :['enjoyment']\\n\", 'use.n.22:00': \"(economics) the utilization of economic goods to satisfy needs or in manufacturing, synonyms :['consumption', 'economic_consumption', 'usance', 'use_of_goods_and_services']\\n\", 'use.v.34:00': \"use up, consume fully, synonyms :['expend']\\n\", 'use.v.34:01': \"put into service; make work or employ for a particular purpose or for its inherent or natural purpose, synonyms :['utilize', 'utilise', 'apply', 'employ']\\n\", 'use.v.34:02': \"take or consume (regularly or habitually), synonyms :['habituate']\\n\", 'use.v.41:03': 'habitually do something (use only in the past tense), synonyms :[]\\n', 'use.v.41:04': \"avail oneself to, synonyms :['practice', 'apply']\\n\", 'use.v.41:14': \"seek or achieve an end by using to one's advantage, synonyms :[]\\n\"}\n"
     ]
    }
   ],
   "source": [
    "meanings = retrieve_meanings(\"use\", list_of_lists)\n",
    "print(meanings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the sentence without <wsd> token and the index\n",
    "def extract_word_and_index(sentence):\n",
    "    # Find the start and end index of the <WSD> tags\n",
    "    start_index = sentence.find('<WSD>')    \n",
    "    end_index = sentence.find('</WSD>')\n",
    "    \n",
    "\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        # Extract the word between <WSD> tags\n",
    "        word = sentence[start_index + len('<WSD>'):end_index]\n",
    "\n",
    "        # Remove <WSD> and </WSD> tags from the sentence\n",
    "        cleaned_sentence = sentence[:start_index] + word+\" \" + sentence[end_index + len('</WSD>'):]\n",
    "            \n",
    "        return cleaned_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to identify the WSD word from the given sentence and return the WSD word on a sentence\n",
    "def wsdword(text):\n",
    "    match = re.search(r'<WSD>(.*?)</WSD>', text)\n",
    "    if match:\n",
    "        word_inside_wsd = match.group(1)\n",
    "        return word_inside_wsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to counts the number of tokens : input and output tokens\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_tokens(text):\n",
    "    # Tokenize the input text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Count the tokens in the input\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Deshan\n",
      "[nltk_data]     Sumanathilaka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Word lemmatizer to get the base word of the WSD word\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_dictionary_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            data_dict = json.load(file)\n",
    "        return data_dict\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the file doesn't exist\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle the case where the file contains invalid JSON\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(input_char):\n",
    "    pos_mapping = {\n",
    "        'n': 'NOUN',\n",
    "        'v': 'VERB',\n",
    "        'a': 'ADJ',\n",
    "        'r': 'ADV',\n",
    "        's': 'SADV'\n",
    "    }\n",
    "    \n",
    "    return pos_mapping.get(input_char.lower(), 'Invalid input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dict = load_dictionary_from_file(\"my_dictionary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low Freequency Sampling\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_balanced_examples_low(input_list):\n",
    "    # Parse the input list into a dictionary with senseid as key\n",
    "    senseid_dict = defaultdict(list)\n",
    "    \n",
    "    for item in input_list:\n",
    "        #print(item)\n",
    "        try:\n",
    "            example, senseid = item.rsplit(' . ', 1)\n",
    "            #print(senseid.strip())\n",
    "            senseid_dict[senseid.strip()].append(item.strip())  # Store the full item (example + senseid)\n",
    "        except:\n",
    "            continue\n",
    "    #Find the minimum number of examples for any senseid\n",
    "    min_examples = min(len(examples) for examples in senseid_dict.values())\n",
    "    \n",
    "    #Randomly sample min_examples from each senseid\n",
    "    balanced_list = []\n",
    "    for senseid, examples in senseid_dict.items():\n",
    "        balanced_list.extend(random.sample(examples, min_examples))\n",
    "    \n",
    "    return balanced_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = examples=loaded_dict[\"bank\"][\"NOUN\"]\n",
    "print(len(examples))\n",
    "examples = get_balanced_Average_examples(examples)\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_balanced_Average_examples(input_list):\n",
    "    # Parse the input list into a dictionary with senseid as key\n",
    "    senseid_dict = defaultdict(list)\n",
    "    \n",
    "    for item in input_list:\n",
    "        try:\n",
    "            example, senseid = item.rsplit(' . ', 1)\n",
    "            senseid_dict[senseid.strip()].append(item.strip())  # Store the full item\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "    if not senseid_dict:\n",
    "        return []\n",
    "\n",
    "    # Calculate the average number of examples\n",
    "    sense_lengths = [len(examples) for examples in senseid_dict.values()]\n",
    "    avg_examples = int((min(sense_lengths) + max(sense_lengths)) / 2)\n",
    "\n",
    "    # Randomly sample from each senseid\n",
    "    balanced_list = []\n",
    "    for senseid, examples in senseid_dict.items():\n",
    "        # Ensure the sample size is not larger than the number of available examples\n",
    "        sample_size = min(avg_examples, len(examples))\n",
    "        balanced_list.extend(random.sample(examples, sample_size))\n",
    "    \n",
    "    return balanced_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sense_Tag_Return_pipeline(sentence,postag,wordwsd):\n",
    "    \n",
    "    meanings = retrieve_meanings(wordwsd, list_of_lists)\n",
    "\n",
    "    cleaned_sentence= extract_word_and_index(sentence) \n",
    "    #retrieving the data form the dictionary\n",
    "    filtered_definitions = {key: value for key, value in meanings.items() if postag in key}\n",
    "    \n",
    "    if wordwsd in loaded_dict:\n",
    "        #print(\"examples found\")\n",
    "        examples=loaded_dict[wordwsd]\n",
    "        try:\n",
    "            examples=examples[get_pos(postag)]\n",
    "            examples = get_balanced_Average_examples(examples)\n",
    "        except:\n",
    "            examples=examples\n",
    "        \n",
    "\n",
    "    elif word_base(wordwsd) in loaded_dict:\n",
    "        #print(\"examples found\")\n",
    "        examples=loaded_dict[word_base(wordwsd)]\n",
    "        try:\n",
    "            examples=examples[get_pos(postag)]\n",
    "            examples = get_balanced_Average_examples(examples)\n",
    "        except:\n",
    "            examples=examples\n",
    "    else:\n",
    "        examples=None\n",
    "        \n",
    "\n",
    "    count=0 #variable to count the tokens \n",
    "    \n",
    "    #prompt=f\"Examine the sentence. {instance_meaning}.Return most suitable sense id associated with from below. it contain sense id and it's definition {meanings}. utilize the below examples also to finalize the answer {examples}\"\n",
    "    prompt = f'''You are going to identify the corresponding sense tag of an ambiguous word in English sentences. Use multiple reasoning strategies to increase confidence in your answer.\n",
    "1. The word \"{wordwsd}\" has different meanings. Below are possible meanings. Comprehend the sense tags and meanings. Synonyms are provided if available. {filtered_definitions}\n",
    "2. You can learn more on the usage of each word and the its sense through the examples below. Each sentence is followed by its corresponding sense id. \"{examples}\"\n",
    "3. Now carefully examine the sentence below. The ambiguous word is enclosed within <WSD>.\"{sentence}\"\n",
    "4. Analyze the sentence using the following techniques and identify the meaning of the ambiguous word.\n",
    "   Focus on keywords in the sentence surrounding the ambiguous word. \n",
    "   Think about the overall topic and intent of the sentence. Decide on the sense of the word that makes the most logical sense within the context. \n",
    "5. Based on the identified meaning, try to find the most appropriate senseIDs from the below sense tag list. You are given definition of each sense tag too.\"{filtered_definitions}\".\n",
    "6. If you have more than one senseIDs identified after above steps, you can return the senseIDs in order of confident level, follow the given format to return the value .\n",
    "7.Return only the finalized senseIDs. Do not add extra detials and explanation. Only senseIDS expected.\n",
    "\n",
    "if only one senseid is identified then <senseId>\n",
    "if not <senseId1, senseId2,...> '''\n",
    "    #print(prompt)\n",
    "    count+=count_tokens(prompt)\n",
    "    try:\n",
    "        output=chat_completion(prompt)\n",
    "        count+=count_tokens(output)\n",
    "        return output, count  \n",
    "    \n",
    "    except:\n",
    "        prompt = f'''You are going to identify the corresponding sense tag of an ambiguous word in English sentences. Use multiple reasoning strategies to increase confidence in your answer.\n",
    "1. The word \"{wordwsd}\" has different meanings. Below are possible meanings. Comprehend the sense tags and meanings. Synonyms are provided if available. {filtered_definitions}\n",
    "2. Now carefully examine the sentence below. The ambiguous word is enclosed within <WSD>.\"{sentence}\"\n",
    "3. Analyze the sentence using the following techniques and identify the meaning of the ambiguous word.\n",
    "   Focus on keywords in the sentence surrounding the ambiguous word. \n",
    "   Think about the overall topic and intent of the sentence. Decide on the sense of the word that makes the most logical sense within the context. \n",
    "4. Based on the identified meaning, try to find the most appropriate senseIDs from the below sense tag list. You are given definition of each sense tag too.\"{filtered_definitions}\".\n",
    "5. If you have more than one senseIDs identified after above steps, you can return the senseIDs in order of confident level, follow the given format to return the value .\n",
    "6.Return only the finalized senseIDs. Do not add extra detials and explanation. Only senseIDS expected.\n",
    "\n",
    "if only one senseid is identified then <senseId>\n",
    "if not <senseId1, senseId2,...>'''\n",
    "        \n",
    "\n",
    "        output=chat_completion(prompt)\n",
    "        count+=count_tokens(output)\n",
    "        return output, count\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average frequncy Sampling :GPT 4o\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "#evaluating the results\n",
    "no_of_instances=0\n",
    "no_of_correct_instances=0\n",
    "\n",
    "file=open(\"test/semeval2013/semeval2013sample.txt\",\"r\",encoding=\"utf8\")\n",
    "\n",
    "\n",
    "totalTokens=0\n",
    "for i in file:\n",
    "    try:\n",
    "        \n",
    "        lst=i.split(\"\t\")\n",
    "        sentence,senseid=lst[0],lst[1]\n",
    "        postag=senseid.split(\".\")[1] #postag \n",
    "        wordfromtext=senseid.split(\".\")[0] #wsdword\n",
    "        #print(postag)\n",
    "        output,token_count=sense_Tag_Return_pipeline(sentence,postag,wordfromtext)\n",
    "        totalTokens+=token_count        \n",
    "        print(output.replace(\"\\n\", \"\"))\n",
    "        no_of_instances+=1\n",
    "        if senseid in output:\n",
    "            no_of_correct_instances+=1\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "file.close()\n",
    "print(totalTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low frequncy Sampling :GPT 4o\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "#evaluating the results\n",
    "no_of_instances=0\n",
    "no_of_correct_instances=0\n",
    "\n",
    "file=open(\"test/semeval2013/semeval2013sample.txt\",\"r\",encoding=\"utf8\")\n",
    "\n",
    "\n",
    "totalTokens=0\n",
    "for i in file:\n",
    "    try:\n",
    "        \n",
    "        lst=i.split(\"\t\")\n",
    "        sentence,senseid=lst[0],lst[1]\n",
    "        postag=senseid.split(\".\")[1] #postag \n",
    "        wordfromtext=senseid.split(\".\")[0] #wsdword\n",
    "        #print(postag)\n",
    "        output,token_count=sense_Tag_Return_pipeline(sentence,postag,wordfromtext)\n",
    "        totalTokens+=token_count        \n",
    "        print(output.replace(\"\\n\", \"\"))\n",
    "        no_of_instances+=1\n",
    "        if senseid in output:\n",
    "            no_of_correct_instances+=1\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "file.close()\n",
    "print(totalTokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#highest frequncy Sampling :GPT 4o\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "#evaluating the results\n",
    "no_of_instances=0\n",
    "no_of_correct_instances=0\n",
    "\n",
    "file=open(\"test/semeval2013/semeval2013sample.txt\",\"r\",encoding=\"utf8\")\n",
    "\n",
    "\n",
    "totalTokens=0\n",
    "for i in file:\n",
    "    try:\n",
    "        \n",
    "        lst=i.split(\"\t\")\n",
    "        sentence,senseid=lst[0],lst[1]\n",
    "        postag=senseid.split(\".\")[1] #postag \n",
    "        wordfromtext=senseid.split(\".\")[0] #wsdword\n",
    "        #print(postag)\n",
    "        output,token_count=sense_Tag_Return_pipeline(sentence,postag,wordfromtext)\n",
    "        totalTokens+=token_count        \n",
    "        print(output.replace(\"\\n\", \"\"))\n",
    "        no_of_instances+=1\n",
    "        if senseid in output:\n",
    "            no_of_correct_instances+=1\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "file.close()\n",
    "print(totalTokens)\n",
    "\n",
    "print(totalTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average frequncy Sampling : LLama\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "#evaluating the results\n",
    "no_of_instances=0\n",
    "no_of_correct_instances=0\n",
    "\n",
    "file=open(\"test/semeval2013/semeval2013sample.txt\",\"r\",encoding=\"utf8\")\n",
    "\n",
    "\n",
    "totalTokens=0\n",
    "for i in file:\n",
    "    try:\n",
    "        \n",
    "        lst=i.split(\"\t\")\n",
    "        sentence,senseid=lst[0],lst[1]\n",
    "        postag=senseid.split(\".\")[1] #postag \n",
    "        wordfromtext=senseid.split(\".\")[0] #wsdword\n",
    "        #print(postag)\n",
    "        output,token_count=sense_Tag_Return_pipeline(sentence,postag,wordfromtext)\n",
    "        totalTokens+=token_count        \n",
    "        print(output.replace(\"\\n\", \"\"))\n",
    "        no_of_instances+=1\n",
    "        if senseid in output:\n",
    "            no_of_correct_instances+=1\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "file.close()\n",
    "print(totalTokens)\n",
    "\n",
    "print(totalTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#highest frequncy Sampling : Llama\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "#evaluating the results\n",
    "no_of_instances=0\n",
    "no_of_correct_instances=0\n",
    "\n",
    "file=open(\"test/semeval2013/semeval2013sample.txt\",\"r\",encoding=\"utf8\")\n",
    "\n",
    "\n",
    "totalTokens=0\n",
    "for i in file:\n",
    "    try:\n",
    "        \n",
    "        lst=i.split(\"\t\")\n",
    "        sentence,senseid=lst[0],lst[1]\n",
    "        postag=senseid.split(\".\")[1] #postag \n",
    "        wordfromtext=senseid.split(\".\")[0] #wsdword\n",
    "        #print(postag)\n",
    "        output,token_count=sense_Tag_Return_pipeline(sentence,postag,wordfromtext)\n",
    "        totalTokens+=token_count        \n",
    "        print(output.replace(\"\\n\", \"\"))\n",
    "        no_of_instances+=1\n",
    "        if senseid in output:\n",
    "            no_of_correct_instances+=1\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "file.close()\n",
    "print(totalTokens)\n",
    "\n",
    "print(totalTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowest sampling : LlaMa\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "#evaluating the results\n",
    "no_of_instances=0\n",
    "no_of_correct_instances=0\n",
    "\n",
    "file=open(\"test/semeval2013/semeval2013sample.txt\",\"r\",encoding=\"utf8\")\n",
    "\n",
    "\n",
    "totalTokens=0\n",
    "for i in file:\n",
    "    try:\n",
    "        \n",
    "        lst=i.split(\"\t\")\n",
    "        sentence,senseid=lst[0],lst[1]\n",
    "        postag=senseid.split(\".\")[1] #postag \n",
    "        wordfromtext=senseid.split(\".\")[0] #wsdword\n",
    "        #print(postag)\n",
    "        output,token_count=sense_Tag_Return_pipeline(sentence,postag,wordfromtext)\n",
    "        totalTokens+=token_count        \n",
    "        print(output.replace(\"\\n\", \"\"))\n",
    "        no_of_instances+=1\n",
    "        if senseid in output:\n",
    "            no_of_correct_instances+=1\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "file.close()\n",
    "print(totalTokens)\n",
    "\n",
    "print(\"No of Total instance :\",no_of_instances)\n",
    "print(\"No of correct instance :\",no_of_correct_instances)\n",
    "print(totalTokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
