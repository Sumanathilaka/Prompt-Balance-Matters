{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating German WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, re\n",
    "import together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "os.environ['TOGETHER_AI_API'] = credentials['TOGETHER_AI_API']\n",
    "together.api_key = os.environ[\"TOGETHER_AI_API\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_model(USER_MESSAGE):\n",
    "    output = together.Complete.create(\n",
    "                                    prompt= USER_MESSAGE,\n",
    "                                    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "                                    max_tokens=512,\n",
    "                                    temperature=0,\n",
    "                                    top_p=0.7,\n",
    "                                    top_k=50,\n",
    "                                    repetition_penalty=1,\n",
    "                                    stop=[\"<|eot_id|>\"]\n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                    )\n",
    "    text = output['output']['choices'][0]['text']\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download fr_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "\n",
    "wordnet_dict={}\n",
    "with open (\"inventory.fr.txt\",\"r\") as file:\n",
    "    for i in file:\n",
    "        parts=i.split(\"\\t\")\n",
    "        element=i.split(\"\\t\")[0]\n",
    "        word,pos=element.split(\"#\")[0],element.split(\"#\")[1]\n",
    "        value = \"\\t\".join(parts[1:]).strip()\n",
    "        doc = nlp(word)\n",
    "        lemmatized_words = [token.lemma_ for token in doc]\n",
    "\n",
    "\n",
    "        #print(lemmatized_words[0]+\"\\t\"+pos+\"\\t\"+value)\n",
    "\n",
    "        # meanings = get_babelnet_sense_meaning(sense_id)\n",
    "        # print(meanings[0])\n",
    "\n",
    "        #save all the data in a dictionary keeping the word as key and the senses as values in a list\n",
    "        wordnet_dict[word]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_babelnet_sense_meaning(sense_id):\n",
    "    # Define the BabelNet API endpoint\n",
    "    url = f'https://babelnet.io/v6/getSynset'\n",
    "    \n",
    "    # Define the parameters for the API request\n",
    "    params = {\n",
    "        'id': sense_id,\n",
    "        'key': \"#\"\n",
    "    }\n",
    "    \n",
    "    # Make the API request\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Get the English glosses (definitions)\n",
    "        #print(data)\n",
    "        glosses = data.get('glosses', [])\n",
    "        english_glosses = [gloss['gloss'] for gloss in glosses if gloss['language'] == 'EN']\n",
    "\n",
    "        lemmas = []\n",
    "        for sense in data.get('senses', []):\n",
    "            properties = sense.get('properties', {})\n",
    "            lemma_info = properties.get('lemma', {})\n",
    "            lemma = lemma_info.get('lemma')\n",
    "            if lemma:\n",
    "                lemmas.append(lemma)\n",
    "\n",
    "\n",
    "        senses = data.get('senses', [])\n",
    "        senseKey = None\n",
    "\n",
    "        if senses:\n",
    "            sense_properties = senses[0].get('properties', {})\n",
    "            senseKey = sense_properties.get('senseKey', None)\n",
    "\n",
    "\n",
    "        #print(lemmas)\n",
    "\n",
    "        return english_glosses[0],lemmas[0],senseKey\n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch data (status code: {response.status_code})\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# glosses,lemma,sensekey_wn=get_babelnet_sense_meaning(\"bn:00003651n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sloping gallery with seats for spectators (as in an operating room or theater)\n",
      "amphitheater\n",
      "amphitheater%1:06:01::\n"
     ]
    }
   ],
   "source": [
    "# print(glosses)\n",
    "# print(lemma)\n",
    "# print(sensekey_wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file\n",
    "wordnet = []\n",
    "with open('wordnet.txt', 'r',encoding=\"utf8\") as file:\n",
    "    for element in file:\n",
    "        items=element.strip(\"\\n\").split(\"\t\")\n",
    "        wordnet.append(items)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fourth_element(lists_of_lists, key):\n",
    "    for sublist in lists_of_lists:\n",
    "        if sublist[0] == key:\n",
    "            return sublist[3]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = extract_fourth_element(wordnet, sensekey_wn)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# from collections import defaultdict\n",
    "\n",
    "# def get_balanced_examples(input_list):\n",
    "#     # Parse the input list into a dictionary with senseid as key\n",
    "#     senseid_dict = defaultdict(list)\n",
    "    \n",
    "#     for item in input_list:\n",
    "        \n",
    "#         example, senseid = item.rsplit(',', 1)\n",
    "#         senseid_dict[senseid.strip()].append(item.strip())  # Store the full item (example + senseid)\n",
    "    \n",
    "#     #Find the minimum number of examples for any senseid\n",
    "#     min_examples = min(len(examples) for examples in senseid_dict.values())\n",
    "    \n",
    "#     #Randomly sample min_examples from each senseid\n",
    "#     balanced_list = []\n",
    "#     for senseid, examples in senseid_dict.items():\n",
    "#         balanced_list.extend(random.sample(examples, min_examples))\n",
    "    \n",
    "#     return balanced_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_balanced_examples(input_list):\n",
    "    # Parse the input list into a dictionary with senseid as key\n",
    "    senseid_dict = defaultdict(list)\n",
    "    \n",
    "    for item in input_list:\n",
    "        \n",
    "        example, senseid = item.rsplit(',', 1)\n",
    "        senseid_dict[senseid.strip()].append(item.strip())  # Store the full item (example + senseid)\n",
    "    \n",
    "    #Find the minimum number of examples for any senseid\n",
    "    avg_examples = min(len(examples) for examples in senseid_dict.values()) + max(len(examples) for examples in senseid_dict.values())\n",
    "    avg_examples=int(avg_examples/2)\n",
    "\n",
    "    \n",
    "    #Randomly sample min_examples from each senseid\n",
    "    balanced_list = []\n",
    "    for senseid, examples in senseid_dict.items():\n",
    "        balanced_list.extend(random.sample(examples, avg_examples))\n",
    "    \n",
    "    return balanced_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to counts the number of tokens : input and output tokens\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_tokens(text):\n",
    "    # Tokenize the input text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Count the tokens in the input\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_dictionary_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            data_dict = json.load(file)\n",
    "        return data_dict\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the file doesn't exist\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle the case where the file contains invalid JSON\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dict = load_dictionary_from_file(\"my_dictionary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying the WSD word from the data \n",
    "def wsdword(text):\n",
    "    match = re.search(r'<WSD>(.*?)<WSD>', text)\n",
    "    if match:\n",
    "        word_inside_wsd = match.group(1)\n",
    "        return word_inside_wsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sense_Tag_Return_pipeline(sentence,postag,wordwsd,wordnet):\n",
    "    \n",
    "    #retrieving the base form of the sentence\n",
    "    doc = nlp(wordwsd)\n",
    "    lemmatized_words = [token.lemma_ for token in doc]\n",
    "    lemmatized_word =lemmatized_words[0].lower()\n",
    "    #print(lemmatized_word)\n",
    "    #retrieving the data form the KB (retriver)\n",
    "    try:\n",
    "        examples=loaded_dict[wordwsd][postag]\n",
    "        #Fewshot bias Removal\n",
    "        examples=get_balanced_examples(examples)\n",
    "\n",
    "\n",
    "        #print(examples)\n",
    "    except:\n",
    "        examples=\"No Examples\"\n",
    "        #print(examples)\n",
    "\n",
    "    #retrieving the meanings from Bablenet\n",
    "    sense_definition=\"\"\n",
    "    list_senseids=wordnet_dict[lemmatized_word].split(\"\\t\")\n",
    "    #print(list_senseids)\n",
    "    for i in list_senseids:\n",
    "        glosses,lemma,sensekey_wn=get_babelnet_sense_meaning(i)\n",
    "        result = extract_fourth_element(wordnet, sensekey_wn)\n",
    "        senseinfo=\"senseid: \"+i+\", gloss :\"+glosses+ \", lemma in English :\"+lemma +\", Synonyms of the word if any :\"+result+\"\\n\"\n",
    "        sense_definition+=senseinfo\n",
    "\n",
    "   \n",
    "    #print(sense_definition)\n",
    "    \n",
    "        \n",
    "\n",
    "    count=0 #variable to count the tokens \n",
    "    \n",
    "    #prompt=f\"Examine the sentence. {instance_meaning}.Return most suitable sense id associated with from below. it contain sense id and it's definition {meanings}. utilize the below examples also to finalize the answer {examples}\"\n",
    "    prompt = f'''You are going to identify the corresponding sense tag of an ambiguous word in french sentences. Use multiple reasoning strategies to increase confidence in your answer.\n",
    "1. The word \"{wordwsd}\" has different meanings. Below are possible meanings. Comprehend the sense tags and meanings. English lemma and synonyms are provided. {sense_definition}\n",
    "2. You can learn more on the usage of each word and the its sense through the examples below. Each sentence is followed by its corresponding sense id. \"{examples}\"\n",
    "3. Now carefully examine the German sentence below. The ambiguous word is enclosed within <WSD>.\"{sentence}\"\n",
    "4. Analyze the sentence using the following techniques and identify the meaning of the ambiguous word.\n",
    "   Focus on keywords in the sentence surrounding the ambiguous word. \n",
    "   Think about the overall topic and intent of the sentence. Decide on the sense of the word that makes the most logical sense within the context. \n",
    "5. Based on the identified meaning, try to find the most appropriate senseID from the above sense tag list. You are given definition of each sense tag too.\n",
    "6. Return only the finalized senseID as a JSON object. Do not add extra details and explanation. Only senseID as a JSON is expected.\n",
    "   > '''\n",
    "    #print(prompt)\n",
    "    count+=count_tokens(prompt)\n",
    "    output=complete_model(prompt)\n",
    "    count+=count_tokens(output)\n",
    "    return output, count    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "#evaluating the results\n",
    "no_of_instances=0\n",
    "no_of_correct_instances=0\n",
    "\n",
    "csv_file_path = 'test/sem13_fr.csv'\n",
    "totalTokens=0\n",
    "# Open the CSV file\n",
    "with open(csv_file_path, mode='r', newline='' , encoding=\"utf-8\") as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    \n",
    "    # Iterate over each row in the CSV file\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        # Access the columns by their header names\n",
    "        sentence = row['sentence']\n",
    "        fileid = row['fileid']\n",
    "        senseid = row['senseid'].strip()        \n",
    "        \n",
    "        try:\n",
    "            postag=senseid[-1]  #postag \n",
    "            wordfromtext=wsdword(sentence)\n",
    "            #print(wordfromtext+\" \"+senseid+\" \"+postag+\" \"+sentence)            \n",
    "            output,token_count=sense_Tag_Return_pipeline(sentence,postag,wordfromtext.lower(),wordnet)\n",
    "            #sense_Tag_Return_pipeline(sentence,postag,wordfromtext)\n",
    "            totalTokens+=token_count\n",
    "            print(output.replace(\"\\n\", \"\"))\n",
    "            no_of_instances+=1\n",
    "            if senseid in output:\n",
    "                no_of_correct_instances+=1\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            print()\n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "file.close()\n",
    "print(\"No of Total instance :\",no_of_instances)\n",
    "print(\"No of correct instance :\",no_of_correct_instances)\n",
    "print(totalTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "#evaluating the results\n",
    "no_of_instances=0\n",
    "no_of_correct_instances=0\n",
    "\n",
    "csv_file_path = 'test/sem13_fr.csv'\n",
    "totalTokens=0\n",
    "# Open the CSV file\n",
    "with open(csv_file_path, mode='r', newline='' , encoding=\"utf-8\") as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    \n",
    "    # Iterate over each row in the CSV file\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        # Access the columns by their header names\n",
    "        sentence = row['sentence']\n",
    "        fileid = row['fileid']\n",
    "        senseid = row['senseid'].strip()        \n",
    "        \n",
    "        try:\n",
    "            postag=senseid[-1]  #postag \n",
    "            wordfromtext=wsdword(sentence)\n",
    "            #print(wordfromtext+\" \"+senseid+\" \"+postag+\" \"+sentence)            \n",
    "            output,token_count=sense_Tag_Return_pipeline(sentence,postag,wordfromtext.lower(),wordnet)\n",
    "            #sense_Tag_Return_pipeline(sentence,postag,wordfromtext)\n",
    "            totalTokens+=token_count\n",
    "            print(output.replace(\"\\n\", \"\"))\n",
    "            no_of_instances+=1\n",
    "            if senseid in output:\n",
    "                no_of_correct_instances+=1\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            print()\n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "file.close()\n",
    "print(\"No of Total instance :\",no_of_instances)\n",
    "print(\"No of correct instance :\",no_of_correct_instances)\n",
    "print(totalTokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
